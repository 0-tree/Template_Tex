\documentclass{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% style template inspired from Overleaf's arXiv template:
% https://www.overleaf.com/latex/templates/style-and-template-for-preprints-arxiv-bio-arxiv/vknsbpqnxqsk
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{arxiv_light} % see this for how to import cleanly : https://tex.stackexchange.com/questions/1137/where-do-i-place-my-own-sty-or-cls-files-to-make-them-available-to-all-my-te

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}


\title{A Map of Entropy Related Concepts}

\author{Arthur Roullier}

\begin{document}
\maketitle

% abstract can be removed
% \begin{abstract}
% \lipsum[1]
% \end{abstract}

% keywords can be removed
% \keywords{First keyword \and Second keyword \and More}


\section{Entropy of a random variable / a distribution}
Discrete case.\\
Let $X$ be a random variable over $\{x_0, \dots, x_n\}$ with distribution $p^X$ such that $p^X(X=x_i) = p^X_i$.\\
The entropy of $p^X$ (or $X$) is

\begin{eqnarray*}
    H(X) = H(p^X) =& E_X[-\log(p(X))] \\
                =& - \sum_{i=0}^n p^X_i \log(p^X_i)
\end{eqnarray*}

\section{Joint entropy of a vector of random variables / a joint distribution}
Discrete case. Two variables: generalization straightforward. \\
Let $X,Y$ be  random variables over $\{x_0, \dots, x_n\}$ and $\{y_0, \dots, y_m\}$ with joint distribution $p$ such that $p(X=x_i,Y=y_j) = p_{i,j}$.\\
The entropy of $p$ (or the vector $[X,Y]$) is

\begin{eqnarray*}
    H([X,Y]) = H(p) =& E_{X,Y}[-\log(p(X,Y))] \\
                    =& - \sum_{i=0}^n \sum_{j=0}^m p_{i,j} \log(p_{i,j})
\end{eqnarray*}


\section{Mutual information between two random variables / two  distributions}
Discrete case.\\
Let $X,Y$ be  random variables over $\{x_0, \dots, x_n\}$ and $\{y_0, \dots, y_m\}$ with joint distribution $p$ such that $p(X=x_i,Y=y_j) = p_{i,j}$, and marginal distributions $p^X$ and $p^Y$ such that $p^X(X = x_i) = p^X_i$ and $p^Y(Y = y_j) = p^Y_j$.\\
The mutual information between $X$ and $Y$ (I would also say between marginals $p^X$ and $p^Y$) is

\begin{eqnarray*}
    I([X,Y]) = I(p) =& E_{X,Y}[-\log(\frac{p^X(X)p^Y(Y)}{p(X,Y)})] \\
                    =& - \sum_{i=0}^n \sum_{j=0}^m p_{i,j} \log(\frac{p^X_i p^Y_j}{p_{i,j}})
\end{eqnarray*}



\section{Kullback-Leibler divergence (a.k.a. relative entropy) between two distributions}

Let $X,Y$ be  random variables over $\{z_0, \dots, z_n\}$  with  marginal distributions $p^X$ and $p^Y$ such that $p^X(X = z_i) = p^X_i$ and $p^Y(Y = z_i) = p^Y_i$.\\
The Kullback-Leibler divergence between $p^X$ and $p^Y$ is

\begin{eqnarray*}
    KL(p^X||p^Y) =& E_{X}[-\log(\frac{p^Y(X)}{p^X(X)})] \\
                    =& - \sum_{i=0}^n  p^X_{i} \log(\frac{p^Y_i}{p^X_i})
\end{eqnarray*}


\section{Cross-entropy between two random variables / two distributions}

\section{Log-likelihood of XXX}

\section{Logistic function}

\section{Logistic regression}


\section{Links}

Let $X,Y$ be  random variables over $\{z_0, \dots, z_n\}$  with joint distribution $p$ such that $p(X=z_i,Y=z_j)=p_{i,j}$ and marginal distributions $p^X$ and $p^Y$ such that $p^X(X = z_i) = p^X_i$ and $p^Y(Y = z_i) = p^Y_i$.\\
We have

\begin{eqnarray*}
    I([X,Y]) = I(p) =& E_{X,Y}[-\log(\frac{p^X(X)p^Y(Y)}{p(X,Y)})] \\
             =& KL(p||p^X p^Y) \\
             =&  - \sum_{i=0}^n \sum_{j=0}^m p_{i,j} \log(\frac{p^X_i p^Y_j}{p_{i,j}})\\
             =& - \sum_{i=0}^n \sum_{j=0}^m p^X(X=z_i)p^Y(Y=z_j|X=x_i) \log(\frac{p^X_i p^Y_j}{p_{i,j}})\\
             =& - \sum_{i=0}^n p^X(X=z_i) \sum_{j=0}^m p^Y(Y=z_j|X=x_i) \log(\frac{p^X_i p^Y_j}{p_{i,j}})\\
\end{eqnarray*}


I am here.

% \section{Entropy of a random variable / its distribution}
% Discrete case.\\
% Let $X$ be a random variable over $\{x_0, \dots, x_n\}$ with distribution $p$ such that $p(X=x_i) = p_i$. The entropy of $p$ (or $X$) is

% \begin{eqnarray*}
%     H(X) = H(p) =& E_X[-\log(p(X))] \\
%                 =& - \sum_{i=0}^n p_i \log(p_i)
% \end{eqnarray*}



% \section{Headings: first level}
% \label{sec:headings}

% \lipsum[4] See Section \ref{sec:headings}.

% \subsection{Headings: second level}
% \lipsum[5]
% \begin{equation}
% \xi _{ij}(t)=P(x_{t}=i,x_{t+1}=j|y,v,w;\theta)= {\frac {\alpha _{i}(t)a^{w_t}_{ij}\beta _{j}(t+1)b^{v_{t+1}}_{j}(y_{t+1})}{\sum _{i=1}^{N} \sum _{j=1}^{N} \alpha _{i}(t)a^{w_t}_{ij}\beta _{j}(t+1)b^{v_{t+1}}_{j}(y_{t+1})}}
% \end{equation}

% \subsubsection{Headings: third level}
% \lipsum[6]

% \paragraph{Paragraph}
% \lipsum[7]

% \section{Examples of citations, figures, tables, references}
% \label{sec:others}
% \lipsum[8] \cite{kour2014real,kour2014fast} and see \cite{hadash2018estimate}.

% The documentation for \verb+natbib+ may be found at
% \begin{center}
%   \url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}
% \end{center}
% Of note is the command \verb+\citet+, which produces citations
% appropriate for use in inline text.  For example,
% \begin{verbatim}
%   \citet{hasselmo} investigated\dots
% \end{verbatim}
% produces
% \begin{quote}
%   Hasselmo, et al.\ (1995) investigated\dots
% \end{quote}

% \begin{center}
%   \url{https://www.ctan.org/pkg/booktabs}
% \end{center}


% \subsection{Figures}
% \lipsum[10] 
% See Figure \ref{fig:fig1}. Here is how you add footnotes. \footnote{Sample of the first footnote.}
% \lipsum[11] 

% \begin{figure}
%   \centering
%   \fbox{\rule[-.5cm]{4cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
%   \caption{Sample figure caption.}
%   \label{fig:fig1}
% \end{figure}

% \subsection{Tables}
% \lipsum[12]
% See awesome Table~\ref{tab:table}.

% \begin{table}
%  \caption{Sample table title}
%   \centering
%   \begin{tabular}{lll}
%     \toprule
%     \multicolumn{2}{c}{Part}                   \\
%     \cmidrule(r){1-2}
%     Name     & Description     & Size ($\mu$m) \\
%     \midrule
%     Dendrite & Input terminal  & $\sim$100     \\
%     Axon     & Output terminal & $\sim$10      \\
%     Soma     & Cell body       & up to $10^6$  \\
%     \bottomrule
%   \end{tabular}
%   \label{tab:table}
% \end{table}

% \subsection{Lists}
% \begin{itemize}
% \item Lorem ipsum dolor sit amet
% \item consectetur adipiscing elit. 
% \item Aliquam dignissim blandit est, in dictum tortor gravida eget. In ac rutrum magna.
% \end{itemize}



% \bibliographystyle{unsrt}
% \bibliography{references}

\end{document}
